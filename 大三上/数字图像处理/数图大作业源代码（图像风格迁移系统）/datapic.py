import settings
import tensorflow as tf
import gradio as gr
import os
import numpy as np
from tqdm import tqdm
from model import NeuralStyleTransferModel
import utils
from PIL import Image  
import io

js = """
function createGradioAnimation() {
    var container = document.createElement('div');
    container.id = 'gradio-animation';
    container.style.fontSize = '2em';
    container.style.fontWeight = 'bold';
    container.style.textAlign = 'center';
    container.style.marginBottom = '20px';

    var text = 'æ¬¢è¿ä½¿ç”¨AIè‰ºæœ¯ç…§ç½‘ç«™';
    for (var i = 0; i < text.length; i++) {
        (function(i){
            setTimeout(function(){
                var letter = document.createElement('span');
                letter.style.opacity = '0';
                letter.style.transition = 'opacity 0.5s';
                letter.innerText = text[i];

                container.appendChild(letter);

                setTimeout(function() {
                    letter.style.opacity = '1';
                }, 50);
            }, i * 250);
        })(i);
    }

    var gradioContainer = document.querySelector('.gradio-container');
    gradioContainer.insertBefore(container, gradioContainer.firstChild);

    return 'Animation created';
}
"""


def image_demo1(è¾“å…¥é£æ ¼å›¾ç‰‡, è¾“å…¥):
    # åˆ›å»ºæ¨¡å‹
    model = NeuralStyleTransferModel()
    # åŠ è½½å†…å®¹å›¾ç‰‡
    è¾“å…¥ = tf.convert_to_tensor(è¾“å…¥)
    content_image = utils.load_images(è¾“å…¥)
    # é£æ ¼å›¾ç‰‡
    è¾“å…¥é£æ ¼å›¾ç‰‡ = tf.convert_to_tensor(è¾“å…¥é£æ ¼å›¾ç‰‡)
    style_image = utils.load_images(è¾“å…¥é£æ ¼å›¾ç‰‡)

    # è®¡ç®—å‡ºç›®æ ‡å†…å®¹å›¾ç‰‡çš„å†…å®¹ç‰¹å¾å¤‡ç”¨
    target_content_features = model([content_image, ])['content']
    # è®¡ç®—ç›®æ ‡é£æ ¼å›¾ç‰‡çš„é£æ ¼ç‰¹å¾
    target_style_features = model([style_image, ])['style']

    M = settings.WIDTH * settings.HEIGHT
    N = 3


    def _compute_content_loss(noise_features, target_features):
        """
        è®¡ç®—æŒ‡å®šå±‚ä¸Šä¸¤ä¸ªç‰¹å¾ä¹‹é—´çš„å†…å®¹loss
        :param noise_features: å™ªå£°å›¾ç‰‡åœ¨æŒ‡å®šå±‚çš„ç‰¹å¾
        :param target_features: å†…å®¹å›¾ç‰‡åœ¨æŒ‡å®šå±‚çš„ç‰¹å¾
        """
        content_loss = tf.reduce_sum(tf.square(noise_features - target_features))
        # è®¡ç®—ç³»æ•°
        x = 2. * M * N
        content_loss = content_loss / x
        return content_loss


    def compute_content_loss(noise_content_features):
        """
        è®¡ç®—å¹¶å½“å‰å›¾ç‰‡çš„å†…å®¹loss
        :param noise_content_features: å™ªå£°å›¾ç‰‡çš„å†…å®¹ç‰¹å¾
        """
        # åˆå§‹åŒ–å†…å®¹æŸå¤±
        content_losses = []
        # åŠ æƒè®¡ç®—å†…å®¹æŸå¤±
        for (noise_feature, factor), (target_feature, _) in zip(noise_content_features, target_content_features):
            layer_content_loss = _compute_content_loss(noise_feature, target_feature)
            content_losses.append(layer_content_loss * factor)
        return tf.reduce_sum(content_losses)


    def gram_matrix(feature):
        """
        è®¡ç®—ç»™å®šç‰¹å¾çš„æ ¼æ‹‰å§†çŸ©é˜µ
        """
        # å…ˆäº¤æ¢ç»´åº¦ï¼ŒæŠŠchannelç»´åº¦æåˆ°æœ€å‰é¢
        x = tf.transpose(feature, perm=[2, 0, 1])
        # reshapeï¼Œå‹ç¼©æˆ2d
        x = tf.reshape(x, (x.shape[0], -1))
        # è®¡ç®—xå’Œxçš„é€†çš„ä¹˜ç§¯
        return x @ tf.transpose(x)


    def _compute_style_loss(noise_feature, target_feature):
        """
        è®¡ç®—æŒ‡å®šå±‚ä¸Šä¸¤ä¸ªç‰¹å¾ä¹‹é—´çš„é£æ ¼loss
        :param noise_feature: å™ªå£°å›¾ç‰‡åœ¨æŒ‡å®šå±‚çš„ç‰¹å¾
        :param target_feature: é£æ ¼å›¾ç‰‡åœ¨æŒ‡å®šå±‚çš„ç‰¹å¾
        """
        noise_gram_matrix = gram_matrix(noise_feature)
        style_gram_matrix = gram_matrix(target_feature)
        style_loss = tf.reduce_sum(tf.square(noise_gram_matrix - style_gram_matrix))
        # è®¡ç®—ç³»æ•°
        x = 4. * (M ** 2) * (N ** 2)
        return style_loss / x


    def compute_style_loss(noise_style_features):
        """
        è®¡ç®—å¹¶è¿”å›å›¾ç‰‡çš„é£æ ¼loss
        :param noise_style_features: å™ªå£°å›¾ç‰‡çš„é£æ ¼ç‰¹å¾
        """
        style_losses = []
        for (noise_feature, factor), (target_feature, _) in zip(noise_style_features, target_style_features):
            layer_style_loss = _compute_style_loss(noise_feature, target_feature)
            style_losses.append(layer_style_loss * factor)
        return tf.reduce_sum(style_losses)


    def total_loss(noise_features):
        """
        è®¡ç®—æ€»æŸå¤±
        :param noise_features: å™ªå£°å›¾ç‰‡ç‰¹å¾æ•°æ®
        """
        content_loss = compute_content_loss(noise_features['content'])
        style_loss = compute_style_loss(noise_features['style'])
        return content_loss * settings.CONTENT_LOSS_FACTOR + style_loss * settings.STYLE_LOSS_FACTOR


    # ä½¿ç”¨Admaä¼˜åŒ–å™¨
    optimizer = tf.keras.optimizers.Adam(settings.LEARNING_RATE)

    # åŸºäºå†…å®¹å›¾ç‰‡éšæœºç”Ÿæˆä¸€å¼ å™ªå£°å›¾ç‰‡
    noise_image = tf.Variable((content_image + np.random.uniform(-0.2, 0.2, (1, settings.HEIGHT, settings.WIDTH, 3))) / 2)


    # ä½¿ç”¨tf.functionåŠ é€Ÿè®­ç»ƒ
    @tf.function
    def train_one_step():
        """
        ä¸€æ¬¡è¿­ä»£è¿‡ç¨‹
        """
        # æ±‚loss
        with tf.GradientTape() as tape:
            noise_outputs = model(noise_image)
            loss = total_loss(noise_outputs)
        # æ±‚æ¢¯åº¦
        grad = tape.gradient(loss, noise_image)
        # æ¢¯åº¦ä¸‹é™ï¼Œæ›´æ–°å™ªå£°å›¾ç‰‡
        optimizer.apply_gradients([(grad, noise_image)])
        return loss


    # åˆ›å»ºä¿å­˜ç”Ÿæˆå›¾ç‰‡çš„æ–‡ä»¶å¤¹
    if not os.path.exists(settings.OUTPUT_DIR):
        os.mkdir(settings.OUTPUT_DIR)

    # å…±è®­ç»ƒsettings.EPOCHSä¸ªepochs
    for epoch in range(settings.EPOCHS):
        # ä½¿ç”¨tqdmæç¤ºè®­ç»ƒè¿›åº¦
        with tqdm(total=settings.STEPS_PER_EPOCH, desc='Epoch {}/{}'.format(epoch + 1, settings.EPOCHS)) as pbar:
            # æ¯ä¸ªepochè®­ç»ƒsettings.STEPS_PER_EPOCHæ¬¡
            for step in range(settings.STEPS_PER_EPOCH):
                _loss = train_one_step()
                pbar.set_postfix({'loss': '%.4f' % float(_loss)})
                pbar.update(1)

    image_mean = tf.constant([0.485, 0.456, 0.406])
    image_std = tf.constant([0.299, 0.224, 0.225])
    x = tf.reshape(noise_image, noise_image.shape[1:])
    x = x * image_std + image_mean
    x = x * 255.
    x = tf.cast(x, tf.int32)
    x = tf.clip_by_value(x, 0, 255)
    x = x.numpy()
    return x

demo1_examples=[
    [os.path.join(os.path.dirname(__file__), "images/style_1.jpg")],
    [os.path.join(os.path.dirname(__file__), "images/style_2.jpg")],
    [os.path.join(os.path.dirname(__file__), "images/style_3.jpg")],
    [os.path.join(os.path.dirname(__file__), "images/style_4.jpg")],
    [os.path.join(os.path.dirname(__file__), "images/style_5.jpg")],
    [os.path.join(os.path.dirname(__file__), "images/style_6.jpg")],
]

def image_demo2(è¾“å…¥):
    a = np.dot(è¾“å…¥[..., :3], [0.2989, 0.5870, 0.1140])
    depth = 10.                        # (0-100)  
    grad = np.gradient(a)              #å–å›¾åƒç°åº¦çš„æ¢¯åº¦å€¼  
    grad_x, grad_y =grad               #åˆ†åˆ«å–æ¨ªçºµå›¾åƒæ¢¯åº¦å€¼  
    grad_x = grad_x*depth/100.  
    grad_y = grad_y*depth/100.  
    A = np.sqrt(grad_x**2 + grad_y**2 + 1.)  
    uni_x = grad_x/A  
    uni_y = grad_y/A  
    uni_z = 1./A  
    vec_el = np.pi/2.2                   # å…‰æºçš„ä¿¯è§†è§’åº¦ï¼Œå¼§åº¦å€¼  
    vec_az = np.pi/4.                    # å…‰æºçš„æ–¹ä½è§’åº¦ï¼Œå¼§åº¦å€¼  
    dx = np.cos(vec_el)*np.cos(vec_az)   #å…‰æºå¯¹x è½´çš„å½±å“  
    dy = np.cos(vec_el)*np.sin(vec_az)   #å…‰æºå¯¹y è½´çš„å½±å“  
    dz = np.sin(vec_el)                  #å…‰æºå¯¹z è½´çš„å½±å“  
    b = 255*(dx*uni_x + dy*uni_y + dz*uni_z)     #å…‰æºå½’ä¸€åŒ–  
    b = b.clip(0,255)  
    im = Image.fromarray(b.astype('uint8'))      #é‡æ„å›¾åƒ 
    return im

def image_demo3(è¾“å…¥):
    image = Image.fromarray(è¾“å…¥)
    image.save('temp.png')
    # å®šä¹‰å­—ç¬¦ç”»ä¸­æ‰€ä½¿ç”¨çš„å­—ç¬¦é›†åˆ
    ascii_char = list("1234567890abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!@#$%^&*()_+-=[]{}|;':\",./<>?//  ")

    # å°†256ä¸ªç°åº¦å€¼æ˜ å°„åˆ°70ä¸ªå­—ç¬¦ä¸Š
    def get_char(r, g, b, alpha=256):
        length = len(ascii_char)
        gray = int(0.2126 * r + 0.7152 * g + 0.0722 * b)
        if gray >= 240:
            return ' '
        unit = (256.0 + 1) / length
        return ascii_char[int(gray / unit)]

    # å°†å›¾ç‰‡è½¬æ¢ä¸ºå­—ç¬¦ç”»
    def image_to_ascii(image_path, width=100, save_path=None):
        im = Image.open(image_path)
        im = im.resize((width, int(width * im.size[1] / im.size[0])))
        txt = ""
        for i in range(im.size[1]):
            for j in range(im.size[0]):
                txt += get_char(*im.getpixel((j, i)))
            txt += '\n'
        
        if save_path:
            with open(save_path, 'w') as f:
                f.write(txt)
        
        return txt
    ascii_text = image_to_ascii('temp.png', save_path='å­—ç¬¦ç”».txt')
    return 'å­—ç¬¦ç”».txt'

demo1 = gr.Interface(
    fn=image_demo1, 
    inputs=["image", "image"], 
    outputs="image", 
    title="ç…§ç‰‡é£æ ¼åŒ–",
    examples=demo1_examples,
)

demo2 = gr.Interface(
    fn=image_demo2, 
    inputs="image", 
    outputs="image", 
    title="ç…§ç‰‡ç´ æ",
)

demo3 = gr.Interface(
    fn=image_demo3, 
    inputs="image", 
    outputs="file", 
    title="ç…§ç‰‡å­—ç¬¦åŒ–", 
    description="è¯·è¾“å…¥å¦‚ç¤ºä¾‹ä¸€æ ·åŒºåˆ†æ˜æ˜¾çš„ç…§ç‰‡",
    examples=[os.path.join(os.path.dirname(__file__), "images/leifeng.jpg")],
)

demo = gr.TabbedInterface(
    [demo1, demo2, demo3], 
    ["ç…§ç‰‡é£æ ¼åŒ–", "ç…§ç‰‡ç´ æ", "ç…§ç‰‡å­—ç¬¦åŒ–"], 
    title = "ğŸ¤–AI è‰ºæœ¯ç…§ç›¸é¦†",
    css="""
        h2 {
            text-align: center !important;
        }

        footer {
            visibility: hidden;
        }
    """,
    theme=gr.themes.Base(),
)

if __name__ == "__main__":
    demo.launch(
        auth=("ahwei", '520Wjw')
        #share=True
)
