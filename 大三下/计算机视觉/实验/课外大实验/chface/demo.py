import streamlit as st
from streamlit_webrtc import webrtc_streamer
import av
import cv2
import os
import dlib
import changeface as CF
import base64
import face2animal as FA
import numpy as np
from camera_input_live import camera_input_live
from PIL import Image
import tempfile

# æ‰‹åŠ¿è¯†åˆ«ç¿»é¡µ
import time
import hand as htm  
import math 
from ctypes import cast, POINTER  
from comtypes import CLSCTX_ALL  
from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume

# å¤šçº¿ç¨‹
import threading


session_state = st.session_state
session_state['page'] = 'æ˜æ˜Ÿæ¢è„¸'
page = st.sidebar.radio('å¯¼èˆªæ ', ['æ˜æ˜Ÿæ¢è„¸', 'åŠ¨ç‰©å˜è„¸'])



class StarChFace:
    def __init__(self) -> None:
        here = os.path.dirname(os.path.abspath(__file__))
        models_folder_path = os.path.join(here, 'models')
        faces_folder_path = os.path.join(here, 'faces')
        predictor_path = os.path.join(models_folder_path, 'face_landmarks.dat')
        self.hand_detector = htm.handDetector(detectionCon=1)  # åˆ›å»ºä¸€ä¸ªæ‰‹éƒ¨æ£€æµ‹å™¨å¯¹è±¡ï¼Œè®¾å®šæ£€æµ‹ç½®ä¿¡åº¦ä¸º1
        self.trend_queue = htm.TrendQueue(max_size=15) # åˆå§‹åŒ–åæ ‡é˜Ÿåˆ—
        self.detector = dlib.get_frontal_face_detector()
        self.prevX = 0  # åˆå§‹åŒ–ä¸Šä¸€æ¬¡æ‰‹æŒ‡çš„xåæ ‡
        self.predictor = dlib.shape_predictor(predictor_path)
        self.faces_images = CF.ChangeFace.load_face_images(faces_folder_path)
        self.current_face_index = 0
        self.before = True

        # self.im1 = cv2.imread(self.faces_images[self.current_face_index])
        # self.im1 = cv2.resize(self.im1, (600, self.im1.shape[0] * 600 // self.im1.shape[1]))
        # self.landmarks1 = CF.ChangeFace.get_face_landmarks(self.im1, self.detector, self.predictor)
        # self.im1_size = CF.ChangeFace.get_image_size(self.im1)
        # self.im1_mask = CF.ChangeFace.get_face_mask(self.im1_size, self.landmarks1)


    # def process_image(self, frame):
    #     img = frame.to_ndarray(format="bgr24")
    #     landmarks2 = CF.ChangeFace.get_face_landmarks(img, self.detector, self.predictor)
    #     img1 = self.hand_detector.findHands(img)  # åœ¨å›¾åƒä¸Šå¯»æ‰¾å¹¶æ ‡è®°æ‰‹éƒ¨
    #     lmList = self.hand_detector.findPosition(img1, draw=False)  # è·å–æ‰‹éƒ¨å…³é”®ç‚¹çš„ä½ç½®ï¼Œä¸ç»˜åˆ¶æ ‡è®°

    #     im1 = cv2.imread(self.faces_images[self.current_face_index])
    #     im1 = cv2.resize(im1, (600, im1.shape[0] * 600 // im1.shape[1]))
    #     landmarks1 = CF.ChangeFace.get_face_landmarks(im1, self.detector, self.predictor)
    #     im1_size = CF.ChangeFace.get_image_size(im1)
    #     im1_mask = CF.ChangeFace.get_face_mask(im1_size, landmarks1)
    #     if landmarks2 is not None:
    #         im2_size = CF.ChangeFace.get_image_size(img)
    #         im2_mask = CF.ChangeFace.get_face_mask(im2_size, landmarks2)
    #         affine_im1 = CF.ChangeFace.get_affine_image(im1, img, landmarks1, landmarks2)
    #         affine_im1_mask = CF.ChangeFace.get_affine_image(im1_mask, img, landmarks1, landmarks2)
    #         union_mask = CF.ChangeFace.get_mask_union(im2_mask, affine_im1_mask)
    #         seamless_im = CF.ChangeFace.skin_color_adjustment(affine_im1, img, mask=union_mask)
    #         point = CF.ChangeFace.get_mask_center_point(affine_im1_mask)
    #         seamless_im = cv2.seamlessClone(affine_im1, img, mask=union_mask, p=point, flags=cv2.NORMAL_CLONE)
    #         img = seamless_im

    # def process_hand(self, frame):
    #     img = frame.to_ndarray(format="bgr24")
    #     landmarks2 = CF.ChangeFace.get_face_landmarks(img, self.detector, self.predictor)
    #     img1 = self.hand_detector.findHands(img)  # åœ¨å›¾åƒä¸Šå¯»æ‰¾å¹¶æ ‡è®°æ‰‹éƒ¨
    #     lmList = self.hand_detector.findPosition(img1, draw=False)  # è·å–æ‰‹éƒ¨å…³é”®ç‚¹çš„ä½ç½®ï¼Œä¸ç»˜åˆ¶æ ‡è®°

    #     im1 = cv2.imread(self.faces_images[self.current_face_index])
    #     im1 = cv2.resize(im1, (600, im1.shape[0] * 600 // im1.shape[1]))
    #     landmarks1 = CF.ChangeFace.get_face_landmarks(im1, self.detector, self.predictor)
    #     im1_size = CF.ChangeFace.get_image_size(im1)
    #     im1_mask = CF.ChangeFace.get_face_mask(im1_size, landmarks1)
    #     if len(lmList[0]) != 0:
    #         y2 = lmList[0][8][2]  # è·å–ç¬¬äºŒä¸ªæ‰‹æŒ‡å…³èŠ‚çš„åæ ‡
    #         self.trend_queue.push(y2)
    #         now = self.trend_queue.detect_rising_trend()
    #         if now and not self.before:
    #             self.current_face_index = (self.current_face_index + 1) % len(self.faces_images)
    #             print("ç¿»é¡µ")
    #         else:
    #             print("ä¸ç¿»é¡µ")
    #         self.before = now
    
    def recv(self, frame):
        img = frame.to_ndarray(format="bgr24")
        landmarks2 = CF.ChangeFace.get_face_landmarks(img, self.detector, self.predictor)
        img1 = self.hand_detector.findHands(img)  # åœ¨å›¾åƒä¸Šå¯»æ‰¾å¹¶æ ‡è®°æ‰‹éƒ¨
        lmList = self.hand_detector.findPosition(img1, draw=False)  # è·å–æ‰‹éƒ¨å…³é”®ç‚¹çš„ä½ç½®ï¼Œä¸ç»˜åˆ¶æ ‡è®°

        im1 = cv2.imread(self.faces_images[self.current_face_index])
        im1 = cv2.resize(im1, (600, im1.shape[0] * 600 // im1.shape[1]))
        landmarks1 = CF.ChangeFace.get_face_landmarks(im1, self.detector, self.predictor)
        im1_size = CF.ChangeFace.get_image_size(im1)
        im1_mask = CF.ChangeFace.get_face_mask(im1_size, landmarks1)

        if landmarks2 is not None:
            im2_size = CF.ChangeFace.get_image_size(img)
            im2_mask = CF.ChangeFace.get_face_mask(im2_size, landmarks2)
            affine_im1 = CF.ChangeFace.get_affine_image(im1, img, landmarks1, landmarks2)
            affine_im1_mask = CF.ChangeFace.get_affine_image(im1_mask, img, landmarks1, landmarks2)
            union_mask = CF.ChangeFace.get_mask_union(im2_mask, affine_im1_mask)
            seamless_im = CF.ChangeFace.skin_color_adjustment(affine_im1, img, mask=union_mask)
            point = CF.ChangeFace.get_mask_center_point(affine_im1_mask)
            seamless_im = cv2.seamlessClone(affine_im1, img, mask=union_mask, p=point, flags=cv2.NORMAL_CLONE)
            img = seamless_im
        if len(lmList[0]) != 0:  # å¦‚æœæ£€æµ‹åˆ°æ‰‹éƒ¨å…³é”®ç‚¹
            x2, y2 = lmList[0][8][1], lmList[0][8][2]  # è·å–ç¬¬äºŒä¸ªæ‰‹æŒ‡å…³èŠ‚çš„åæ ‡
            cv2.circle(img, (x2, y2), 15, (255, 0, 255), cv2.FILLED)  # ç»˜åˆ¶ç¬¬äºŒä¸ªæ‰‹æŒ‡å…³èŠ‚çš„åœ†åœˆ

            self.trend_queue.push(y2)
            now = self.trend_queue.detect_rising_trend()

            # listener = keyboard.Listener(on_press=on_press)
            # listener.start()
            # listener.join()

            if now and not self.before:
            # if FLAG:
                self.current_face_index = (self.current_face_index + 1) % len(self.faces_images)
                cv2.putText(img, f'page up', (40, 50), cv2.FONT_HERSHEY_COMPLEX, 1, (255, 0, 0), 3)  # åœ¨å›¾åƒä¸Šæ˜¾ç¤ºå¸§ç‡
                FLAG = False
            
            self.before = now
        # # åˆ›å»ºå¹¶å¯åŠ¨ä¸¤ä¸ªçº¿ç¨‹æ¥å¤„ç†å›¾åƒå¤„ç†å’Œæ‰‹éƒ¨å…³é”®ç‚¹å¤„ç†
        # image_thread = threading.Thread(target=StarChFace.process_image(self, frame))
        # hand_thread = threading.Thread(target=StarChFace.process_hand(self, frame))
        # image_thread.start()
        # hand_thread.start()

        # # ç­‰å¾…ä¸¤ä¸ªçº¿ç¨‹æ‰§è¡Œå®Œæ¯•
        # image_thread.join()
        # hand_thread.join()
        return av.VideoFrame.from_ndarray(img, format="bgr24")

class AnimalFace:
    def capture_image():
        camera_image = st.camera_input("")
        if camera_image:
            image = Image.open(camera_image)
            # è¿™é‡Œå¯ä»¥æ·»åŠ ä»£ç å°†imageè½¬æ¢ä¸ºOpenCVæ ¼å¼è¿›è¡Œå¤„ç†ï¼Œä»¥åŠæ‰§è¡ŒåŠ¨ç‰©è„¸éƒ¨æ›¿æ¢çš„é€»è¾‘
            # ç¤ºä¾‹ï¼šå°†PILå›¾åƒè½¬æ¢ä¸ºOpenCVæ ¼å¼
            image_cv = np.array(image)[:, :, ::-1].copy()
            # åœ¨æ­¤å¤„æ·»åŠ ä½ çš„é¢éƒ¨å˜æ¢ä»£ç ï¼Œå¦‚FA.get_two_trianglesç­‰
            # ...
            # å¤„ç†åï¼Œå¦‚æœéœ€è¦ï¼Œå¯ä»¥å°†å›¾åƒè½¬å›PILæ ¼å¼å±•ç¤º
            return image_cv
        return None

def main_bg(main_bg):
    main_bg_ext = "png"
    st.markdown(
        f"""
         <style>
         .stApp {{
             background: url(data:image/{main_bg_ext};base64,{base64.b64encode(open(main_bg, "rb").read()).decode()});
             background-size: cover
         }}
         </style>
         """,
        unsafe_allow_html=True
    )

if page == 'æ˜æ˜Ÿæ¢è„¸':
    st.title('æ˜æ˜Ÿæ¢è„¸')
    main_bg('./pics/bac.png')
    ctx = webrtc_streamer(
        key="example",
        video_processor_factory=StarChFace,
        rtc_configuration={
            "iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]
        }
    )

elif page == 'åŠ¨ç‰©å˜è„¸':
    st.title('åŠ¨ç‰©å˜è„¸')
    st.markdown(
        """
        <style>
        body {
            background-image: url('https://www.baidu.com/img/bdlogo.png');
            background-size: cover;
        }
        </style>
        """,
        unsafe_allow_html=True
    )
    main_bg('./pics/background.png')
    animal = st.radio('', ('çŒ«ğŸˆ', 'ç†ŠçŒ«ğŸ¼', 'çŒ©çŒ©ğŸ¦§'))
    uploaded_file = st.file_uploader("é€‰æ‹©ä¸€å¼ å›¾ç‰‡", type="png")

    if animal:
        if uploaded_file is not None:
            file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)
            img = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)
            img = img[0:400, 0:400]
            if animal == "çŒ«ğŸˆ":
                image, image2, img1_triangles, img2_triangles = FA.get_two_triangles(img, "images\\cat.jpg", "temp.png", "cat.png", np.load("images\\cat_landmarks.npy"))
                FA.write_videos(img1_triangles,img2_triangles,image,image2,"outputs\\temp.mp4")
                video_file = open('outputs\\temp.mp4', 'rb')
                video_bytes = video_file.read()
                st.video(video_bytes)

            elif animal == "ç†ŠçŒ«ğŸ¼":
                image, image2, img1_triangles, img2_triangles = FA.get_two_triangles(img, "images\\panda.jpg", "temp.png", "panda.png", np.load("images\\panda_landmarks.npy"))
                FA.write_videos(img1_triangles,img2_triangles,image,image2,"outputs\\temp.mp4")
                video_file = open('outputs\\temp.mp4', 'rb')
                video_bytes = video_file.read()
                st.video(video_bytes)

            elif animal == "çŒ©çŒ©ğŸ¦§":
                image, image2, img1_triangles, img2_triangles = FA.get_two_triangles(img, "images\\gorilla.jpg", "temp.png", "gorilla.png", np.load("images\\gorilla_landmarks.npy"))
                FA.write_videos(img1_triangles,img2_triangles,image,image2,"outputs\\temp.mp4")
                video_file = open('outputs\\temp.mp4', 'rb')
                video_bytes = video_file.read()

                st.video(video_bytes)
